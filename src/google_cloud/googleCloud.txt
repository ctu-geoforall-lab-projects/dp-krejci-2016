#configuration is in /home/hadoop/hive/lib

*********************************************************
#ssh to cluser
*********************************************************
gcloud compute --project "spatial-hadoop" ssh --zone "europe-west1-b" "cluster-1-m"
gcloud --project=spatial-hadoop2 compute ssh --zone=europe-west1-b hadoop-m

*********************************************************
#tunel to cluser
*********************************************************
gcloud compute ssh --zone=europe-west1-b --ssh-flag="-D 1080" --ssh-flag="-N" --ssh-flag="-n" cluster-1-m

#web interface
/usr/bin/google-chrome \
  --proxy-server="socks5://localhost:1080" \
  --host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" \
  --user-data-dir=/tmp/

*********************************************************
#deply new cluster
*********************************************************
1download bdutil
2create bucket

*********************************************************
##init cluster
*********************************************************
gcloud beta dataproc clusters create cluster-2 --zone europe-west1-b --master-machine-type n1-standard-1 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-standard-1 --worker-boot-disk-size 300 --project spatial-hadoop 

##bdutil deploy additional files from command
./bdutil -P spatial-cluster --env_var_files extensions/querytools/querytools_env.sh deploy
##deploy with files included commands
./bdutil -e my_base_env.sh deploy

*********************************************************
#describe cluster
#gcloud dataproc https://cloud.google.com/hadoop/bdutil
*********************************************************
gcloud dataproc clusters describe cluster-1

*********************************************************
#delete cluster
*********************************************************
matt@cluster-1-m:/$ mc ./usr/local/share/google/dataproc/bdutil-dataproc-20160216-151931-RC0
./bdutil delete

*********************************************************
Access data from google starage 
*********************************************************
https://cloud.google.com/storage/docs/access-control#About-Access-Control-Lists

*********************************************************
#Check your quota
*********************************************************
gcloud compute project-info describe --project myproject
gcloud compute regions describe example-region

*********************************************************
#hadoop check config
*********************************************************
hadoop org.apache.hadoop.conf.Configuration | grep webhdfs

*********************************************************
#creating VM for copy data 
*********************************************************
gcloud compute --project "spatial-hadoop" instances create "mwcopy" --zone "europe-west1-b" --machine-type "custom-2-7680" --network "default" --no-restart-on-failure --maintenance-policy "TERMINATE" --scopes default="https://www.googleapis.com/auth/cloud-platform" --image "/ubuntu-os-cloud/ubuntu-1404-trusty-v20160314" --boot-disk-size "60" --boot-disk-type "pd-standard" --boot-disk-device-name "mwcopy"

*********************************************************
#copy slice (CRC32C and Installing crcmod)
********************************************************
#https://cloud.google.com/storage/docs/gsutil/addlhelp/CRC32CandInstallingcrcmod 

sudo apt-get install -y gcc python-dev python-setuptools 
sudo easy_install -U pip
sudo pip uninstall crcmod
sudo pip install -U crcmod

gsutil -m -o GSUtil:parallel_composite_upload_threshold=150M cp gs://mwdata_export/* ./mwdbe
xport


*********************************************************
#add 
********************************************************
gcloud compute --project "spatial-hadoop" firewall-rules create "hadoop" --allow udp:50060-50100 --description "webhdfs to hadoop" --network "default" --source-ranges "0.0.0.0/0"
********************************************************




gcloud compute ssh --zone=europe-west1-b --ssh-flag="-D 1050" --ssh-flag="-N" --ssh-flag="-n" spatial-cluster-m


gcloud --project=spatial-hadoop compute ssh --zone=europe-west1-b --ssh-flag="-D 1089" --ssh-flag="-N" --ssh-flag="-n" cluster-2-m	



  /usr/bin/google-chrome \
  --proxy-server="socks5://localhost:1050" \
  --host-resolver-rules="MAP * 0.0.0.0 , EXCLUDE localhost" \
  --user-data-dir=/tmp/




http://stackoverflow.com/questions/31580832/hdfs-put-vs-webhdfs

CREATE external TABLE IF NOT EXISTS csv(y INT,x INT,Z INT) ROW FORMAT delimited fields terminated by ',' STORED AS textfile;
CREATE external TABLE IF NOT EXISTS tablename (y INT,x INT,Z INT) ROW FORMAT DELIMITED FIELDS TERMINATED BY ';' STORED AS textfile ;








#### nastaveni HIVE
hadoop fs -mkdir -p  /user/hive/warehouse/tablename
hadoop fs -chmod -R g+w  /user





#sed replace
sed -i -e 's/"ring"/"type":"Polygon","coordinates"/g'









gcloud dataproc clusters create cluster-3 --zone us-central1-b --master-machine-type n1-highmem-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-highmem-4 --worker-boot-disk-size 500 --num-worker-local-ssds 2 --image-version 1.0 --scopes 'https://www.googleapis.com/auth/cloud-platform' --project hadoop-jakub  --bucket open_street_asia --properties 'hive:hive.aux.jars.path=file:///usr/local/spatial/jar'


gcloud dataproc clusters create cluster-4 --zone us-central1-b --master-machine-type n1-highmem-4 --master-boot-disk-size 500 --num-workers 2 --worker-machine-type n1-highmem-2 --worker-boot-disk-size 500  --image-version 1.0 --scopes 'https://www.googleapis.com/auth/cloud-platform' --project hadoop-jakub  --properties 'hive:hive.aux.jars.path=file:///usr/lib/hive/auxlib/spatial-framework-for-hadoop/json/target/spatial-sdk-json-1.1.1-SNAPSHOT.jar,file:///usr/lib/hive/auxlib/spatial-framework-for-hadoop/hive/target/spatial-sdk-hive-1.1.1-SNAPSHOT.jar,file:///usr/lib/hive/auxlib/Hive-JSON-Serde/json-serde/target/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar,file:///usr/lib/hive/auxlib/geometry-api-java/target/esri-geometry-api-1.2.1.jar' --bucket open_street