*********************************************************
#set properties of HIVE
*********************************************************
set mapred.reduce.tasks=<number>


*********************************************************
#add jar to hive
*********************************************************
add jar ${env:HIVE_HOME}/lib/esri-geometry-api.jar;
add jar ${env:HIVE_HOME}/lib/spatial-sdk-hadoop.jar;
add jar ${env:HIVE_HOME}/lib/json-serde-1.3.8-SNAPSHOT-jar-with-dependencies.jar;

add jar ${env:HIVE_HOME}/hcatalog/share/hcatalog/hcatalog-core-0.12.0.jar;

*********************************************************
#add function to hive
*********************************************************
create temporary function ST_Buffer as 'com.esri.hadoop.hive.ST_Buffer';
create temporary function ST_LineString as 'com.esri.hadoop.hive.ST_LineString';

*********************************************************
#create table my
*********************************************************
####### TABLE: LINKS1
CREATE EXTERNAL TABLE links1(
    type string,
    properties STRUCT<cat:SMALLINT>,
    geometry string
) ROW FORMAT SERDE 'org.openx.data.jsonserde.JsonSerDe'
STORED AS TEXTFILE;

LOAD DATA LOCAL INPATH '/home/matt/mw_nocomma.json' OVERWRITE INTO TABLE links;
LOAD DATA LOCAL INPATH '/home/matt/mw_comma.json' OVERWRITE INTO TABLE links1;
select geometry from links1;

###
    geometry STRUCT<type:string,coordinates:string>
###

-- A simple schema with several features promoted to fields in the table.
CREATE EXTERNAL TABLE links1_orc(
cat smallint,
geometry string
)
STORED AS ORC;

-- We are casting many features from string to a more appropriate type of smallint as we
-- ingest into the ORC table
INSERT INTO TABLE links1_orc select 
cast(properties.cat as smallint),
geometry from links1 where geometry != 'NULL';

select ST_GeomFromGeoJson(links1_orc.geometry) from links1_orc;

####### TABLE: MWRECORD
https://community.hortonworks.com/articles/2834/when-to-use-and-when-not-to-use-hive-csvserde.html


CREATE EXTERNAL TABLE  mwrecord (
cat smallint, 
date TIMESTAMP, 
tx FLOAT, 
rx FLOAT
)
ROW FORMAT DELIMITED FIELDS TERMINATED BY ',';

LOAD DATA INPATH '/mw_data/mwdump.csv' OVERWRITE INTO TABLE  mwrecord;












*********************************************************
#create table https://community.hortonworks.com/articles/5129/geospatial-data-analysis-in-hadoop.html
*********************************************************
add jar ${env:SPATIAL}/esri-geometry-api.jar ${env:SPATIAL}/spatial-sdk-hadoop.jar ${env:HIVE_HOME}/hcatalog/share/hcatalog/hcatalog-core-0.12.0.jar;

CREATE EXTERNAL TABLE census1 (
type string,
properties map<string,string>,
geometry string
)
ROW FORMAT SERDE 'org.apache.hcatalog.data.JsonSerDe'
STORED AS TEXTFILE;
 
LOAD DATA INPATH '/test_data/2010_census.json' OVERWRITE INTO TABLE  census1;

set hive.execution.engine=mr;
 
-- A simple schema with several features promoted to fields in the table.
CREATE EXTERNAL TABLE census_orc (
name string,
population double,
male double,
female double,
age_0_4 double,
age_5_11 double,
age_12_14 double,
age_15_17 double,
age_18_24 double,
age_25_34 double,
age_35_44 double,
age_45_64 double,
age_65_ovr double,
vacant double,
occupied double,
geometry string
)
STORED AS ORC;
 
-- We are casting many features from string to a more appropriate type of double as we
-- ingest into the ORC table.
INSERT INTO TABLE census_orc select properties['name'],
cast(properties['population'] as double),
cast(properties['Male'] as double),
cast(properties['Female'] as double),
cast(properties['AGE0_4'] as double),
cast(properties['AGE5_11'] as double),
cast(properties['AGE12_14'] as double),
cast(properties['AGE15_17'] as double),
cast(properties['AGE18_24'] as double),
cast(properties['AGE25_34'] as double),
cast(properties['AGE35_44'] as double),
cast(properties['AGE45_65'] as double),
cast(properties['AGE65ovr'] as double),
cast(properties['Vacant'] as double),
cast(properties['Occupied'] as double),
geometry from census_text where geometry != 'NULL';

create temporary function ST_Point as 'com.esri.hadoop.hive.ST_Point';
create temporary function ST_Contains as 'com.esri.hadoop.hive.ST_Contains';
create temporary function ST_GeomFromGeoJson as 'com.esri.hadoop.hive.ST_GeomFromGeoJson';


*********************************************************
#create table thor
*********************************************************

add jar ${env:SPATIAL}/esri-geometry-api.jar ${env:SPATIAL}/spatial-sdk-hadoop.jar ${env:HIVE_HOME}/hcatalog/share/hcatalog/hcatalog-core-0.12.0.jar;

{"foo":"ABC","bar":"20090101100000","quux":{"quuxid":1234,"quuxname":"sam"}}

CREATE TABLE json_serde2 (
  foo string,
  bar string,
  quux struct<quuxid:int, quuxname:string>
)
ROW FORMAT SERDE 'org.apache.hcatalog.data.JsonSerDe';

LOAD DATA LOCAL INPATH '/home/matt/thor.json' INTO TABLE json_serde1;

SELECT foo, bar, quux.quuxid, quux.quuxname
FROM json_serde1;














##configure hive
##google cloud 
# 

<property>
  <name>hive.aux.jars.path</name>
  <value>/usr/local/spatial/jar</value>
</property>

<property>
  <name>hive.aux.jars.path</name>
  <value>/var/lib/hive</value>
</property>




$HADOOP_HOME/bin/hadoop fs -mkdir       /tmp
$HADOOP_HOME/bin/hadoop fs -mkdir -p     /root/hive/warehouse
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /tmp
$HADOOP_HOME/bin/hadoop fs -chmod g+w   /root/hive/warehouse	