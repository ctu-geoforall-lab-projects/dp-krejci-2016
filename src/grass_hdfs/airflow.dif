diff --git a/airflow/hooks/__init__.py b/airflow/hooks/__init__.py
index 58fac17..45f0f34 100644
--- a/airflow/hooks/__init__.py
+++ b/airflow/hooks/__init__.py
@@ -1,37 +1,15 @@
-# Imports the hooks dynamically while keeping the package API clean,
-# abstracting the underlying modules
-
-from airflow.utils.helpers import import_module_attrs as _import_module_attrs
-from airflow.hooks.base_hook import BaseHook  # noqa to expose in package
-
-_hooks = {
-    'hive_hooks': [
-        'HiveCliHook',
-        'HiveMetastoreHook',
-        'HiveServer2Hook',
-    ],
-    'hdfs_hook': ['HDFSHook'],
-    'webhdfs_hook': ['WebHDFSHook'],
-    'pig_hook': ['PigCliHook'],
-    'mysql_hook': ['MySqlHook'],
-    'postgres_hook': ['PostgresHook'],
-    'presto_hook': ['PrestoHook'],
-    'samba_hook': ['SambaHook'],
-    'sqlite_hook': ['SqliteHook'],
-    'S3_hook': ['S3Hook'],
-    'http_hook': ['HttpHook'],
-    'druid_hook': ['DruidHook'],
-    'jdbc_hook': ['JdbcHook'],
-    'dbapi_hook': ['DbApiHook'],
-    'mssql_hook': ['MsSqlHook'],
-    'oracle_hook': ['OracleHook'],
-}
-
-_import_module_attrs(globals(), _hooks)
-
-
-def integrate_plugins():
-    """Integrate plugins to the context"""
-    from airflow.plugins_manager import hooks as _hooks
-    for _h in _hooks:
-        globals()[_h.__name__] = _h
+import os
+
+import base_hook
+import connections
+import hdfs_hook
+import hive_hook
+import security_utils
+import settings
+import webhdfs_hook
+
+for module in os.listdir(os.path.dirname(__file__)):
+    if module == '__init__.py' or module[-3:] != '.py':
+        continue
+    __import__(module[:-3], locals(), globals())
+del module
diff --git a/airflow/hooks/base_hook.py b/airflow/hooks/base_hook.py
index 2a6cb73..72c5daa 100644
--- a/airflow/hooks/base_hook.py
+++ b/airflow/hooks/base_hook.py
@@ -3,39 +3,37 @@ from __future__ import division
 from __future__ import print_function
 from __future__ import unicode_literals
 
-from builtins import object
 import logging
 import os
 import random
 
-from airflow import settings
-from airflow.models import Connection
-from airflow.exceptions import AirflowException
+from builtins import object
+
+from hdfswrapper import settings
+from hdfswrapper.connections import Connection
 
-CONN_ENV_PREFIX = 'AIRFLOW_CONN_'
+CONN_ENV_PREFIX = 'GRASSHIVE_CONN_'
 
 
 class BaseHook(object):
     """
     Abstract base class for hooks, hooks are meant as an interface to
-    interact with external systems. MySqlHook, HiveHook, PigHook return
+    interact with external systems. HDFS,  HiveHook, return
     object that can handle the connection and interaction to specific
     instances of these systems, and expose consistent methods to interact
     with them.
     """
+
     def __init__(self, source):
         pass
 
     @classmethod
     def get_connections(cls, conn_id):
         session = settings.Session()
-        db = (
-            session.query(Connection)
-            .filter(Connection.conn_id == conn_id)
-            .all()
-        )
+
+        db = (session.query(Connection).filter(Connection.conn_id == conn_id).all())
         if not db:
-            raise AirflowException(
+            raise Exception(
                 "The conn_id `{0}` isn't defined".format(conn_id))
         session.expunge_all()
         session.close()
@@ -57,15 +55,3 @@ class BaseHook(object):
     def get_hook(cls, conn_id):
         connection = cls.get_connection(conn_id)
         return connection.get_hook()
-
-    def get_conn(self):
-        raise NotImplementedError()
-
-    def get_records(self, sql):
-        raise NotImplementedError()
-
-    def get_pandas_df(self, sql):
-        raise NotImplementedError()
-
-    def run(self, sql):
-        raise NotImplementedError()
diff --git a/airflow/hooks/hdfs_hook.py b/airflow/hooks/hdfs_hook.py
index 98e5f97..b4ba8d3 100644
--- a/airflow/hooks/hdfs_hook.py
+++ b/airflow/hooks/hdfs_hook.py
@@ -1,16 +1,13 @@
-from airflow.hooks.base_hook import BaseHook
-from airflow import configuration
+from hdfswrapper.base_hook import BaseHook
 
 try:
     snakebite_imported = True
-    from snakebite.client import Client, HAClient, Namenode, AutoConfigClient
+    from snakebite.client import Client, HAClient, Namenode
 except ImportError:
     snakebite_imported = False
 
-from airflow.exceptions import AirflowException
 
-
-class HDFSHookException(AirflowException):
+class HDFSHookException(Exception):
     pass
 
 
@@ -18,6 +15,7 @@ class HDFSHook(BaseHook):
     '''
     Interact with HDFS. This class is a wrapper around the snakebite library.
     '''
+
     def __init__(self, hdfs_conn_id='hdfs_default', proxy_user=None):
         if not snakebite_imported:
             raise ImportError(
@@ -32,31 +30,55 @@ class HDFSHook(BaseHook):
         '''
         Returns a snakebite HDFSClient object.
         '''
-        connections = self.get_connections(self.hdfs_conn_id)
-
         use_sasl = False
-        if configuration.get('core', 'security') == 'kerberos':
+        securityConfig = None
+        if securityConfig == 'kerberos':  # TODO make confugration file for thiw
             use_sasl = True
 
+        connections = self.get_connections(self.hdfs_conn_id)
         client = None
-
-        ''' When using HAClient, proxy_user must be the same, so is ok to always take the first '''
+        # When using HAClient, proxy_user must be the same, so is ok to always take the first
         effective_user = self.proxy_user or connections[0].login
         if len(connections) == 1:
-            autoconfig = connections[0].extra_dejson.get('autoconfig', False)
-            if autoconfig:
-                client = AutoConfigClient(effective_user=effective_user, use_sasl=use_sasl)
-            else:
-                hdfs_namenode_principal = connections[0].extra_dejson.get('hdfs_namenode_principal')
-                client = Client(connections[0].host, connections[0].port,
-                                effective_user=effective_user, use_sasl=use_sasl,
-                                hdfs_namenode_principal=hdfs_namenode_principal)
+            client = Client(connections[0].host, connections[0].port, use_sasl=use_sasl, effective_user=effective_user)
         elif len(connections) > 1:
-            hdfs_namenode_principal = connections[0].extra_dejson.get('hdfs_namenode_principal')
             nn = [Namenode(conn.host, conn.port) for conn in connections]
-            client = HAClient(nn, effective_user=effective_user, use_sasl=use_sasl,
-                              hdfs_namenode_principal=hdfs_namenode_principal)
+            client = HAClient(nn, use_sasl=use_sasl, effective_user=effective_user)
         else:
             raise HDFSHookException("conn_id doesn't exist in the repository")
-
         return client
+
+    def test(self):
+        try:
+            client = self.get_conn()
+            print('***' * 30)
+            print("\n    Test connection (ls /) \n")
+            print('***' * 30)
+            print(type(client.count(['/'])))
+            print('-' * 40 + '\n')
+            return False
+        except Exception, e:
+            print("     EROOR: connection can not be established: %s \n" % e)
+            print('***' * 30)
+            return False
+
+    def download_file(self):
+        raise NotImplementedError
+
+    def mkdir(self):
+        raise NotImplementedError
+
+    def write(self):
+        raise NotImplementedError
+
+    def load_file(self):
+        raise NotImplementedError
+
+    def check_for_path(self):
+        raise NotImplementedError
+
+    def get_cursor(self):
+        raise NotImplementedError
+
+    def execute(self, hql):
+        raise NotImplementedError
diff --git a/airflow/hooks/hive_hook.py b/airflow/hooks/hive_hook.py
index e69de29..c456d6b 100644
--- a/airflow/hooks/hive_hook.py
+++ b/airflow/hooks/hive_hook.py
@@ -0,0 +1,730 @@
+from __future__ import print_function
+
+import csv
+import logging
+import re
+import subprocess
+
+import pyhs2
+from builtins import zip
+from past.builtins import basestring
+from thrift.protocol import TBinaryProtocol
+from thrift.transport import TSocket, TTransport
+
+import security_utils as utils
+from base_hook import BaseHook
+from utils import string2dict, find_ST_fnc
+
+
+class HiveSpatial(object):
+    def execute(self, hql):
+        NotImplementedError()
+
+    def show_tables(self):
+        hql = 'show tables'
+        res = self.execute(hql, True)
+        if res:
+            print('***' * 30)
+            print('   show tables:')
+            for i in res:
+                print('         %s' % i[0])
+            print('***' * 30)
+
+    def add_functions(self, fce_dict, temporary=False):
+        """
+        :param fce_dict:
+        :type fce_dict:
+        :param temporary:
+        :type temporary:
+        :return:
+        :rtype:
+        """
+        hql = ''
+        for key, val in fce_dict.iteritems():
+            if temporary:
+                hql += "CREATE TEMPORARY FUNCTION %s as '%s'\n" % (key, val)
+            else:
+                hql += "CREATE FUNCTION %s as '%s'\n" % (key, val)
+        self.execute(hql)
+
+    def describe_table(self, table, show=False):
+        hql = "DESCRIBE formatted %s" % table
+        out = self.execute(hql, True)
+        if show:
+            for i in out:
+                print(i)
+        return out
+
+    def find_table_location(self, table):
+        out = self.describe_table(table)
+
+        for cell in out:
+            if 'Location:' in cell[0]:
+                logging.info("Location of file in hdfs:  %s" % cell[1])
+                return cell[1]
+
+    def esri_query(self, hsql, temporary=True):
+        STfce = ''
+        ST = find_ST_fnc(hsql)
+        tmp = ''
+        if temporary:
+            tmp = 'temporary'
+
+        for key, vals in ST.iteritems():
+            STfce += "create {tmp} function {key} as '{vals}' \n"
+
+        hql = STfce.format(**locals())
+        logging.info(hql)
+
+        hsqlexe = '%s\n%s' % (STfce, hsql)
+
+        self.execute(hsqlexe)
+
+    def test(self):
+        hql = 'show databases'
+        try:
+            print('***' * 30)
+            res = self.execute(hql, True)
+            print("\n     Test connection (show databases;) \n       %s\n" % res)
+            print('***' * 30)
+            return True
+        except Exception, e:
+            print("     EROOR: connection can not be established:\n       %s\n" % e)
+            print('***' * 30)
+            return False
+
+    def add_jar(self, jar_list, path=False):
+        """
+        Function for adding jars to the hive path.
+        :param jar_list: list of jars
+        :type jar_list: list
+        :param path: if true , jar_list must incliudes path \
+                 to jar, else by default jars must be in ${env:HIVE_HOME}
+        :type path: bool
+        :return:
+        :rtype:
+        """
+        hql = ''
+        for jar in jar_list:
+            if jar:
+
+                if not path:
+                    hql += 'ADD JAR /usr/local/spatial/jar/%s ' % jar
+                else:
+                    hql += 'ADD JAR %s ' % jar
+                logging.info(hql)
+        hql += '\n'
+        return hql
+
+    def create_geom_table(self,
+                          table,
+                          field_dict=None,
+                          struct=None,
+                          serde='org.openx.data.jsonserde.JsonSerDe',
+                          outputformat=None,
+                          stored=None,
+                          external=False,
+                          recreate=False,
+                          filepath=None,
+                          overwrite=None,
+                          ):
+        hql = ''
+
+        if field_dict and field_dict is not isinstance(field_dict, dict):
+            field_dict = string2dict(field_dict)
+            if field_dict is None:
+                print("Attributes are not defined")
+                return None
+
+        logging.info('field_dict: %s' % field_dict)
+        logging.info('type field_dict: %s' % type(field_dict))
+
+        if recreate:
+            self.drop_table(table)
+        if field_dict:
+            fields = ", ".join(
+                [k + ' ' + v for k, v in field_dict.items()])
+        else:
+            fields = struct
+        if outputformat:
+            outputformat = outputformat.replace("'", "")
+        if external:
+            hql += "CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({fields}) "
+        else:
+            hql += "CREATE TABLE IF NOT EXISTS {table} ({fields}) "
+        hql += "ROW FORMAT SERDE '{serde}' "
+        if stored:
+            hql += "STORED AS '{stored}' "
+        else:
+            hql += "STORED AS textfile "
+        if outputformat:
+            hql += "OUTPUTFORMAT '{outputformat}' "
+        hql = hql.format(**locals())
+        logging.info(hql)
+        self.execute(hql)
+        if filepath:
+            self.data2table(filepath, table, overwrite)
+
+    def data2table(self, filepath, table, overwrite, partition=False):
+        """
+
+        :param filepath: path to hdfs data
+        :type filepath: string
+        :param table: name of table
+        :type table: string
+        :param overwrite: if overwrite data in table
+        :type overwrite: bool;
+        :param partition: target partition as a dict of partition columns and values
+        :type partition: dict;
+        :return:
+        :rtype:
+        """
+
+        hql = "LOAD DATA INPATH '{filepath}' "
+        if overwrite:
+            hql += "OVERWRITE "
+        hql += "INTO TABLE {table} "
+
+        if partition:
+            pvals = ", ".join(
+                ["{0}='{1}'".format(k, v) for k, v in partition.items()])
+            hql += "PARTITION ({pvals})"
+        hql = hql.format(**locals())
+        logging.info(hql)
+        self.execute(hql)
+
+    def create_csv_table(
+            self,
+            filepath,
+            table,
+            delimiter=",",
+            field_dict=None,
+            stored=None,
+            outputformat=None,
+            create=True,
+            external=False,
+            overwrite=True,
+            partition=None,
+            tblproperties=None,
+            recreate=False):
+        """
+        Loads a local file into Hive
+
+        Note that the table generated in Hive uses ``STORED AS textfile``
+        which isn't the most efficient serialization format. If a
+        large amount of data is loaded and/or if the tables gets
+        queried considerably, you may want to use this operator only to
+        stage the data into a temporary table before loading it into its
+        final destination using a ``HiveOperator``.
+
+        :param filepath: path to hdfs data
+        :type filepath:
+        :param table:  target Hive table, use dot notation to target a
+            specific database
+        :type table: str
+        :param delimiter:
+        :type delimiter:
+        :param field_dict:
+        :type field_dict:
+        :param overwrite: Overwrite exsiting data in table
+        :type overwrite: bool
+        :param partition: target partition as a dict of partition columns and values
+        :type partition: dict
+        :param recreate: whether to drop and recreate the table at every execution
+        :type recreate: bool
+        :return:
+        :rtype:
+        """
+
+        if field_dict and field_dict is not isinstance(field_dict, dict):
+            field_dict = string2dict(field_dict)
+            if field_dict is None:
+                return None
+
+        if partition and partition is not isinstance(partition, dict):
+            partition = string2dict(partition)
+            if partition is None:
+                return None
+
+        if not delimiter:
+            delimiter = ','
+
+        if stored == 'textfile':
+            stored = None
+
+        if recreate:
+            self.execute("DROP TABLE IF EXISTS %s" % table)
+
+        fields = ", ".join(
+            [k + ' ' + v for k, v in field_dict.items()])
+        hql = ''
+        if external:
+            hql += "CREATE EXTERNAL TABLE IF NOT EXISTS {table} ({fields}) "
+        else:
+            hql += "CREATE TABLE IF NOT EXISTS {table} ({fields}) "
+        if partition:
+            pfields = ",".join(
+                [p + " STRING" for p in partition])
+            hql += "PARTITIONED BY ({pfields}) "
+        hql += "ROW FORMAT DELIMITED "
+        hql += "FIELDS TERMINATED BY '{delimiter}' "
+        if not stored:
+            hql += "STORED AS textfile"
+        else:
+            hql += "STORED AS INPUTFORMAT '{stored}'"
+        if outputformat: hql += "OUTPUTFORMAT '{outputformat}' "
+        if tblproperties: hql += '"{tblproperties}"'
+
+        hql = hql.format(**locals())
+        logging.info(hql)
+        self.execute(hql)
+
+        if filepath:
+            self.data2table(filepath, table, overwrite, partition)
+
+    def drop_table(self, name):
+        self.execute('DROP TABLE IF EXISTS %s' % name)
+
+
+class HiveCliHook(BaseHook, HiveSpatial):
+    """
+    Simple wrapper around the hive CLI.
+
+    It also supports the ``beeline``
+    a lighter CLI that runs JDBC and is replacing the heavier
+    traditional CLI. To enable ``beeline``, set the use_beeline param in the
+    extra field of your connection as in ``{ "use_beeline": true }``
+
+    Note that you can also set default hive CLI parameters using the
+    ``hive_cli_params`` to be used in your connection as in
+    ``{"hive_cli_params": "-hiveconf mapred.job.tracker=some.jobtracker:444"}``
+
+    The extra connection parameter ``auth`` gets passed as in the ``jdbc``
+    connection string as is.
+
+    """
+
+    def __init__(self, hive_cli_conn_id="hive_cli_default", run_as=None):
+        conn = self.get_connection(hive_cli_conn_id)
+        self.hive_cli_params = conn.extra_dejson.get('hive_cli_params', '')
+        self.use_beeline = conn.extra_dejson.get('use_beeline', True)
+        self.auth = conn.extra_dejson.get('auth', 'noSasl')
+        self.conn = conn
+        self.run_as = run_as
+
+    def execute(self, hql, schema=None, verbose=True):
+        """
+        Run an hql statement using the hive cli
+
+        >>> hh = HiveCliHook()
+        >>> result = hh.run_cli("USE default;")
+        >>> ("OK" in result)
+        True
+        """
+
+        conn = self.conn
+        schema = schema or conn.schema
+        if schema:
+            hql = "USE {schema};\n{hql}".format(**locals())
+        import tempfile, os
+
+        tmp_dir = tempfile.gettempdir()
+        if not os.path.isdir(tmp_dir):
+            os.mkdir(tmp_dir)
+        # print(tmp_dir)
+        tmp_dir = '/tmp/'
+        f = open(os.path.join(tmp_dir, 'tmpfile'), 'a')
+        f.write(hql)
+        f.flush()
+        fname = f.name
+        hive_bin = 'hive'
+        cmd_extra = []
+
+        if self.use_beeline:
+            hive_bin = 'beeline'
+            jdbc_url = "jdbc:hive2://{conn.host}:{conn.port}/{conn.schema}"
+            securityConfig = None
+            if securityConfig == 'kerberos':  # TODO make confugration file for thiw
+                template = conn.extra_dejson.get('principal', "hive/_HOST@EXAMPLE.COM")
+                if "_HOST" in template:
+                    template = utils.replace_hostname_pattern(utils.get_components(template))
+
+                proxy_user = ""
+                if conn.extra_dejson.get('proxy_user') == "login" and conn.login:
+                    proxy_user = "hive.server2.proxy.user={0}".format(conn.login)
+                elif conn.extra_dejson.get('proxy_user') == "owner" and self.run_as:
+                    proxy_user = "hive.server2.proxy.user={0}".format(self.run_as)
+
+                jdbc_url += ";principal={template};{proxy_user}"
+            elif self.auth:
+                jdbc_url += ";auth=" + self.auth
+
+            jdbc_url = jdbc_url.format(**locals())
+
+            cmd_extra += ['-u', jdbc_url]
+            if conn.login:
+                cmd_extra += ['-n', conn.login]
+            if conn.password:
+                cmd_extra += ['-p', conn.password]
+
+        hive_cmd = [hive_bin, '-f', fname] + cmd_extra
+
+        if self.hive_cli_params:
+            hive_params_list = self.hive_cli_params.split()
+            hive_cmd.extend(hive_params_list)
+        if verbose:
+            logging.info(" ".join(hive_cmd))
+
+        logging.info('hive_cmd= %s\ntmp_dir= %s' % (hive_cmd, tmp_dir))
+
+        sp = subprocess.Popen(
+            hive_cmd,
+            stdout=subprocess.PIPE,
+            stderr=subprocess.STDOUT,
+            cwd=tmp_dir)
+        self.sp = sp
+        stdout = ''
+        for line in iter(sp.stdout.readline, ''):
+            stdout += line
+            if verbose:
+                logging.info(line.strip())
+        sp.wait()
+
+        if sp.returncode:
+            raise Exception(stdout)
+
+        return stdout
+
+    def show_tables(self):
+        out = self.execute('show tables')
+
+    def test(self):
+
+        try:
+            print("\n     Test connection (show databases;)\n        %s\n" % out)
+            print('***' * 30)
+            out = self.execute('show databases')
+            return True
+        except Exception, e:
+            print("      EROOR: connection can not be established:\n      %s\n" % e)
+            print('***' * 30)
+            return False
+
+    def test_hql(self, hql):
+        """
+        Test an hql statement using the hive cli and EXPLAIN
+        """
+        create, insert, other = [], [], []
+        for query in hql.split(';'):  # naive
+            query_original = query
+            query = query.lower().strip()
+
+            if query.startswith('create table'):
+                create.append(query_original)
+            elif query.startswith(('set ',
+                                   'add jar ',
+                                   'create temporary function')):
+                other.append(query_original)
+            elif query.startswith('insert'):
+                insert.append(query_original)
+        other = ';'.join(other)
+        for query_set in [create, insert]:
+            for query in query_set:
+
+                query_preview = ' '.join(query.split())[:50]
+                logging.info("Testing HQL [{0} (...)]".format(query_preview))
+                if query_set == insert:
+                    query = other + '; explain ' + query
+                else:
+                    query = 'explain ' + query
+                try:
+                    self.execute(query, verbose=False)
+                except Exception as e:
+                    message = e.args[0].split('\n')[-2]
+                    logging.info(message)
+                    error_loc = re.search('(\d+):(\d+)', message)
+                    if error_loc and error_loc.group(1).isdigit():
+                        l = int(error_loc.group(1))
+                        begin = max(l - 2, 0)
+                        end = min(l + 3, len(query.split('\n')))
+                        context = '\n'.join(query.split('\n')[begin:end])
+                        logging.info("Context :\n {0}".format(context))
+                else:
+                    logging.info("SUCCESS")
+
+    def kill(self):
+        if hasattr(self, 'sp'):
+            if self.sp.poll() is None:
+                print("Killing the Hive job")
+                self.sp.kill()
+
+    def drop_table(self, name):
+        self.execute('DROP TABLE IF EXISTS %s' % name)
+
+
+class HiveMetastoreHook(BaseHook):
+    """
+    Wrapper to interact with the Hive Metastore
+    """
+
+    def __init__(self, metastore_conn_id='metastore_default'):
+        self.metastore_conn = self.get_connection(metastore_conn_id)
+        self.metastore = self.get_metastore_client()
+
+    def __getstate__(self):
+        # This is for pickling to work despite the thirft hive client not
+        # being pickable
+        d = dict(self.__dict__)
+        del d['metastore']
+        return d
+
+    def __setstate__(self, d):
+        self.__dict__.update(d)
+        self.__dict__['metastore'] = self.get_metastore_client()
+
+    def get_metastore_client(self):
+        """
+        Returns a Hive thrift client.
+        """
+        from hive_service import ThriftHive
+
+        ms = self.metastore_conn
+        transport = TSocket.TSocket(ms.host, ms.port)
+        transport = TTransport.TBufferedTransport(transport)
+        protocol = TBinaryProtocol.TBinaryProtocol(transport)
+        return ThriftHive.Client(protocol)
+
+    def get_conn(self):
+        return self.metastore
+
+    def check_for_partition(self, schema, table, partition):
+        """
+        Checks whether a partition exists
+
+        >>> hh = HiveMetastoreHook()
+        >>> t = 'streets'
+        >>> hh.check_for_partition('default', t, "ds='2015-01-01'")
+        True
+        """
+        self.metastore._oprot.trans.open()
+        partitions = self.metastore.get_partitions_by_filter(
+            schema, table, partition, 1)
+        self.metastore._oprot.trans.close()
+        if partitions:
+            return True
+        else:
+            return False
+
+    def get_table(self, table_name, db='default'):
+        '''
+        Get a metastore table object
+
+        >>> hh = HiveMetastoreHook()
+        >>> t = hh.get_table(db='default', table_name='streets')
+        >>> t.tableName
+        'static_babynames'
+        >>> [col.name for col in t.sd.cols]
+        ['state', 'year', 'name', 'gender', 'num']
+        '''
+        self.metastore._oprot.trans.open()
+        if db == 'default' and '.' in table_name:
+            db, table_name = table_name.split('.')[:2]
+        table = self.metastore.get_table(dbname=db, tbl_name=table_name)
+        self.metastore._oprot.trans.close()
+        return table
+
+    def get_tables(self, db, pattern='*'):
+        '''
+        Get a metastore table object
+        '''
+        self.metastore._oprot.trans.open()
+        tables = self.metastore.get_tables(db_name=db, pattern=pattern)
+        objs = self.metastore.get_table_objects_by_name(db, tables)
+        self.metastore._oprot.trans.close()
+        return objs
+
+    def get_databases(self, pattern='*'):
+        '''
+        Get a metastore table object
+        '''
+        self.metastore._oprot.trans.open()
+        dbs = self.metastore.get_databases(pattern)
+        self.metastore._oprot.trans.close()
+        return dbs
+
+    def get_partitions(
+            self, schema, table_name, filter=None):
+        '''
+        Returns a list of all partitions in a table. Works only
+        for tables with less than 32767 (java short max val).
+        For subpartitioned table, the number might easily exceed this.
+
+        >>> hh = HiveMetastoreHook()
+        >>> t = 'dmt30'
+        >>> parts = hh.get_partitions(schema='grassdb', table_name=t)
+        >>> len(parts)
+        1
+        >>> parts
+        [{'ds': '2015-01-01'}]
+        '''
+        self.metastore._oprot.trans.open()
+        table = self.metastore.get_table(dbname=schema, tbl_name=table_name)
+        if len(table.partitionKeys) == 0:
+            raise Exception("The table isn't partitioned")
+        else:
+            if filter:
+                parts = self.metastore.get_partitions_by_filter(
+                    db_name=schema, tbl_name=table_name,
+                    filter=filter, max_parts=32767)
+            else:
+                parts = self.metastore.get_partitions(
+                    db_name=schema, tbl_name=table_name, max_parts=32767)
+
+            self.metastore._oprot.trans.close()
+            pnames = [p.name for p in table.partitionKeys]
+            return [dict(zip(pnames, p.values)) for p in parts]
+
+    def max_partition(self, schema, table_name, field=None, filter=None):
+        '''
+        Returns the maximum value for all partitions in a table. Works only
+        for tables that have a single partition key. For subpartitioned
+        table, we recommend using signal tables.
+
+        >>> hh = HiveMetastoreHook()
+        >>> t = 'static_babynames_partitioned'
+        >>> hh.max_partition(schema='default', table_name=t)
+        '2015-01-01'
+        '''
+        parts = self.get_partitions(schema, table_name, filter)
+        if not parts:
+            return None
+        elif len(parts[0]) == 1:
+            field = list(parts[0].keys())[0]
+        elif not field:
+            raise Exception(
+                "Please specify the field you want the max "
+                "value for")
+
+        return max([p[field] for p in parts])
+
+    def table_exists(self, table_name, db='default'):
+        '''
+        Check if table exists
+
+        >>> hh = HiveMetastoreHook()
+        >>> hh.table_exists(db='hivedb', table_name='static_babynames')
+        True
+        >>> hh.table_exists(db='hivedb', table_name='does_not_exist')
+        False
+        '''
+        try:
+            t = self.get_table(table_name, db)
+            return True
+        except Exception as e:
+            return False
+
+
+class HiveServer2Hook(BaseHook, HiveSpatial):
+    '''
+    Wrapper around the pyhs2 library
+
+    Note that the default authMechanism is NOSASL, to override it you
+    can specify it in the ``extra`` of your connection in the UI as in
+    ``{"authMechanism": "PLAIN"}``. Refer to the pyhs2 for more details.
+    '''
+
+    def __init__(self, hiveserver2_conn_id='hiveserver2_default'):
+        self.hiveserver2_conn_id = hiveserver2_conn_id
+
+    def get_conn(self):
+        db = self.get_connection(self.hiveserver2_conn_id)
+        # auth_mechanism = db.extra_dejson.get('authMechanism', 'SASL')
+        auth_mechanism = 'PLAIN'  # TODO
+
+        securityConfig = None
+        if securityConfig == 'kerberos':  # TODO make confugration file for this
+            auth_mechanism = db.extra_dejson.get('authMechanism', 'KERBEROS')
+
+        return pyhs2.connect(
+            host=str(db.host),
+            port=int(db.port),
+            authMechanism=str(auth_mechanism),
+            user=str(db.login),
+            password=str(db.password),
+            database=str(db.schema))
+
+    def get_results(self, hql, schema='default', arraysize=1000):
+
+        with self.get_conn() as conn:
+            if isinstance(hql, basestring):
+                hql = [hql]
+            results = {
+                'data': [],
+                'header': [],
+            }
+            for statement in hql:
+                with conn.cursor() as cur:
+                    cur.execute(statement)
+                    records = cur.fetchall()
+                    if records:
+                        results = {
+                            'data': records,
+                            'header': cur.getSchema(),
+                        }
+            return results
+
+    def to_csv(self,
+               hql,
+               csv_filepath,
+               schema='default',
+               delimiter=',',
+               lineterminator='\r\n',
+               output_header=True):
+
+        schema = schema or 'default'
+        with self.get_conn() as conn:
+            with conn.cursor() as cur:
+                logging.info("Running query: " + hql)
+                cur.execute(hql)
+                schema = cur.getSchema()
+                with open(csv_filepath, 'w') as f:
+                    writer = csv.writer(f, delimiter=delimiter,
+                                        lineterminator=lineterminator)
+                    if output_header:
+                        writer.writerow([c['columnName']
+                                         for c in cur.getSchema()])
+                    i = 0
+                    while cur.hasMoreRows:
+                        rows = [row for row in cur.fetchmany() if row]
+                        writer.writerows(rows)
+                        i += len(rows)
+                        logging.info("Written {0} rows so far.".format(i))
+                    logging.info("Done. Loaded a total of {0} rows.".format(i))
+
+    def get_records(self, hql, schema='default'):
+        """
+        Get a set of records from a Hive query.
+
+        >>> hh = HiveServer2Hook()
+        >>> sql = "SELECT * FROM default.static_babynames LIMIT 100"
+        >>> len(hh.get_records(sql))
+        100
+        """
+        return self.get_results(hql, schema=schema)['data']
+
+    def get_cursor(self):
+        conn = self.get_conn()
+        return conn.cursor()
+
+    def execute(self, hql, fatch=False):
+        with self.get_conn() as conn:
+            with conn.cursor() as cur:
+                logging.info("Running query: " + hql)
+                try:
+                    cur.execute(hql)
+
+                except Exception, e:
+                    print("Execute error: %s" % e)
+                    return None
+                if fatch:
+                    return cur.fetchall()
diff --git a/airflow/hooks/security_utils.py b/airflow/hooks/security_utils.py
index e69de29..ac504c9 100644
--- a/airflow/hooks/security_utils.py
+++ b/airflow/hooks/security_utils.py
@@ -0,0 +1,70 @@
+#!/usr/bin/env python
+# Licensed to Cloudera, Inc. under one
+# or more contributor license agreements.  See the NOTICE file
+# distributed with this work for additional information
+# regarding copyright ownership.  Cloudera, Inc. licenses this file
+# to you under the Apache License, Version 2.0 (the
+# "License"); you may not use this file except in compliance
+# with the License.  You may obtain a copy of the License at
+#
+#     http://www.apache.org/licenses/LICENSE-2.0
+#
+# Unless required by applicable law or agreed to in writing, software
+# distributed under the License is distributed on an "AS IS" BASIS,
+# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
+# See the License for the specific language governing permissions and
+# limitations under the License.
+#
+
+import re
+import socket
+
+# Pattern to replace with hostname
+HOSTNAME_PATTERN = '_HOST'
+
+
+def get_kerberos_principal(principal, host):
+    components = get_components(principal)
+    if not components or len(components) != 3 or components[1] != HOSTNAME_PATTERN:
+        return principal
+    else:
+        if not host:
+            raise IOError("Can't replace %s pattern since host is null." % HOSTNAME_PATTERN)
+        return replace_hostname_pattern(components, host)
+
+
+def get_components(principal):
+    """
+    get_components(principal) -> (short name, instance (FQDN), realm)
+    ``principal`` is the kerberos principal to parse.
+    """
+    if not principal:
+        return None
+    return re.split('[\/@]', str(principal))
+
+
+def replace_hostname_pattern(components, host=None):
+    fqdn = host
+    if not fqdn or fqdn == '0.0.0.0':
+        fqdn = get_localhost_name()
+    return '%s/%s@%s' % (components[0], fqdn.lower(), components[2])
+
+
+def get_localhost_name():
+    return socket.getfqdn()
+
+
+def get_fqdn(hostname_or_ip=None):
+    # Get hostname
+    try:
+        if hostname_or_ip:
+            fqdn = socket.gethostbyaddr(hostname_or_ip)[0]
+        else:
+            fqdn = get_localhost_name()
+    except IOError:
+        fqdn = hostname_or_ip
+
+    if fqdn == 'localhost':
+        fqdn = get_localhost_name()
+
+    return fqdn
diff --git a/airflow/hooks/settings.py b/airflow/hooks/settings.py
index e69de29..eea73f5 100644
--- a/airflow/hooks/settings.py
+++ b/airflow/hooks/settings.py
@@ -0,0 +1,54 @@
+from __future__ import absolute_import
+from __future__ import division
+from __future__ import print_function
+from __future__ import unicode_literals
+
+import logging
+import os
+import sys
+
+import grass.script as grass
+from sqlalchemy import create_engine
+from sqlalchemy.orm import scoped_session, sessionmaker
+
+BASE_LOG_URL = 'log'
+# SQL_ALCHEMY_CONN = 'sqlite:////home/matt/Dropbox/DIPLOMKA/sqlitedb.db'
+GISDBASE = grass.gisenv()['GISDBASE']
+LOCATION_NAME = grass.gisenv()['LOCATION_NAME']
+MAPSET = grass.gisenv()['MAPSET']
+MAPSET_PATH = os.path.join(GISDBASE, LOCATION_NAME, MAPSET)
+
+SQL_ALCHEMY_CONN = 'sqlite:////%s' % os.path.join(MAPSET_PATH, 'sqlite', 'sqlite.db')
+
+LOGGING_LEVEL = logging.INFO
+
+engine_args = {}
+if 'sqlite' not in SQL_ALCHEMY_CONN:
+    # Engine args not supported by sqlite
+    engine_args['pool_size'] = 5
+    engine_args['pool_recycle'] = 3600
+
+# print(SQL_ALCHEMY_CONN)
+engine = create_engine(SQL_ALCHEMY_CONN, **engine_args)
+
+Session = scoped_session(
+    sessionmaker(autocommit=False, autoflush=False, bind=engine))
+
+LOG_FORMAT = (
+    '[%(asctime)s] {%(filename)s:%(lineno)d} %(levelname)s - %(message)s')
+SIMPLE_LOG_FORMAT = '%(asctime)s %(levelname)s - %(message)s'
+
+grass_config = os.path.join(MAPSET_PATH, 'grasshdfs.conf')
+
+
+# print(grass_config)
+
+
+
+def configure_logging():
+    logging.root.handlers = []
+    logging.basicConfig(
+        format=LOG_FORMAT, stream=sys.stdout, level=LOGGING_LEVEL)
+
+
+configure_logging()
diff --git a/airflow/hooks/utils.py b/airflow/hooks/utils.py
index e69de29..978e06d 100644
--- a/airflow/hooks/utils.py
+++ b/airflow/hooks/utils.py
@@ -0,0 +1,61 @@
+import errno
+import json
+import shutil
+from tempfile import mkdtemp
+
+try:
+    from cryptography.fernet import Fernet
+except:
+    pass
+
+
+def TemporaryDirectory(suffix='', prefix=None, dir=None):
+    name = mkdtemp(suffix=suffix, prefix=prefix, dir=dir)
+    try:
+        yield name
+    finally:
+        try:
+            shutil.rmtree(name)
+        except OSError as e:
+            # ENOENT - no such file or directory
+            if e.errno != errno.ENOENT:
+                raise e
+
+
+def generate_fernet_key():
+    try:
+        FERNET_KEY = Fernet.generate_key().decode()
+    except NameError:
+        FERNET_KEY = "cryptography_not_found_storing_passwords_in_plain_text"
+    return FERNET_KEY
+
+
+def string2dict(string):
+    try:
+        print(string)
+        return json.loads(string.replace("'", '"'))
+
+    except Exception, e:
+        print('Dictonary is not valid: %s' % e)
+        return None
+
+
+def find_ST_fnc(hsql):
+    '''
+    Parse hsql query and find ST_ functions.
+    :param hsql: string of hive query.
+    :type hsql: string
+    :return: dict {ST_fce: com.esri.hadoop.hive.ST_fce} (name: java path )
+    :rtype: dict
+    '''
+    first = "ST_"
+    last = "("
+    ST = {}
+    for s in hsql.split('('):
+        if s.find('ST_'):
+            s = s.split('ST_')
+            fc = 'ST_%s' % s[0]
+            if not fc in ST:
+                ST[s] = "com.esri.hadoop.hive.%s" % fc
+
+    return ST
diff --git a/airflow/hooks/webhdfs_hook.py b/airflow/hooks/webhdfs_hook.py
index 79a23bc..32a297d 100644
--- a/airflow/hooks/webhdfs_hook.py
+++ b/airflow/hooks/webhdfs_hook.py
@@ -1,27 +1,24 @@
-from airflow.hooks.base_hook import BaseHook
-from airflow import configuration
 import logging
+import os
 
 from hdfs import InsecureClient, HdfsError
 
-_kerberos_security_mode = configuration.get("core", "security") == "kerberos"
-if _kerberos_security_mode:
-  try:
-    from hdfs.ext.kerberos import KerberosClient
-  except ImportError:
-    logging.error("Could not load the Kerberos extension for the WebHDFSHook.")
-    raise
-from airflow.exceptions import AirflowException
-
+from base_hook import BaseHook
 
-class AirflowWebHDFSHookException(AirflowException):
-    pass
+_kerberos_security_mode = None  # TODO make confugration file for this
+if _kerberos_security_mode:
+    try:
+        from hdfs.ext.kerberos import KerberosClient
+    except ImportError:
+        logging.error("Could not load the Kerberos extension for the WebHDFSHook.")
+        raise
 
 
 class WebHDFSHook(BaseHook):
     """
     Interact with HDFS. This class is a wrapper around the hdfscli library.
     """
+
     def __init__(self, webhdfs_conn_id='webhdfs_default', proxy_user=None):
         self.webhdfs_conn_id = webhdfs_conn_id
         self.proxy_user = proxy_user
@@ -36,10 +33,10 @@ class WebHDFSHook(BaseHook):
                 logging.debug('Trying namenode {}'.format(nn.host))
                 connection_str = 'http://{nn.host}:{nn.port}'.format(nn=nn)
                 if _kerberos_security_mode:
-                  client = KerberosClient(connection_str)
+                    client = KerberosClient(connection_str)
                 else:
-                  proxy_user = self.proxy_user or nn.login
-                  client = InsecureClient(connection_str, user=proxy_user)
+                    proxy_user = self.proxy_user or nn.login
+                    client = InsecureClient(connection_str, user=proxy_user)
                 client.status('/')
                 logging.debug('Using namenode {} for hook'.format(nn.host))
                 return client
@@ -48,7 +45,20 @@ class WebHDFSHook(BaseHook):
                               " error: {e.message}".format(**locals()))
         nn_hosts = [c.host for c in nn_connections]
         no_nn_error = "Read operations failed on the namenodes below:\n{}".format("\n".join(nn_hosts))
-        raise AirflowWebHDFSHookException(no_nn_error)
+        raise Exception(no_nn_error)
+
+    def test(self):
+        try:
+            path = self.check_for_path("/")
+            print('***' * 30)
+            print("\n   Test <webhdfs> connection (is path exists: ls /) \n    %s \n" % path)
+            print('***' * 30)
+            return True
+
+        except Exception, e:
+            print("\n     EROOR: connection can not be established: %s" % e)
+            print('***' * 30)
+            return False
 
     def check_for_path(self, hdfs_path):
         """
@@ -57,11 +67,19 @@ class WebHDFSHook(BaseHook):
         c = self.get_conn()
         return bool(c.status(hdfs_path, strict=False))
 
-    def load_file(self, source, destination, overwrite=True, parallelism=1,
-                  **kwargs):
+    def check_for_content(self, hdfs_path, recursive=False):
+
+        c = self.get_conn()
+        return c.list(hdfs_path, status=recursive)
+
+    def progress(self, a, b):
+        # print(a)
+        print('progress: chunk_size %s' % b)
+
+    def upload_file(self, source, destination, overwrite=True, parallelism=1,
+                    **kwargs):
         """
         Uploads a file to HDFS
-
         :param source: Local path to file or folder. If a folder, all the files
           inside of it will be uploaded (note that this implies that folders empty
           of files will not be created remotely).
@@ -75,13 +93,48 @@ class WebHDFSHook(BaseHook):
           `0` (or negative) uses as many threads as there are files.
         :type parallelism: int
         :param \*\*kwargs: Keyword arguments forwarded to :meth:`upload`.
-
-
         """
         c = self.get_conn()
         c.upload(hdfs_path=destination,
                  local_path=source,
                  overwrite=overwrite,
                  n_threads=parallelism,
+                 progress=self.progress,
                  **kwargs)
         logging.debug("Uploaded file {} to {}".format(source, destination))
+
+    def download_file(self, hdfs_path, local_path, overwrite=True, parallelism=1,
+                      **kwargs):
+
+        c = self.get_conn()
+        c.download(hdfs_path=hdfs_path,
+                   local_path=local_path,
+                   overwrite=overwrite,
+                   n_threads=parallelism,
+                   **kwargs)
+
+        logging.debug("Download file {} to {}".format(hdfs_path, local_path))
+
+        if os.path.exists(local_path):
+            return None
+        return local_path
+
+    def mkdir(self, path, **kwargs):
+        c = self.get_conn()
+        c.makedirs(hdfs_path=path, **kwargs)
+
+        logging.debug("Mkdir file {} ".format(path))
+
+    def write(self, fs, hdfs, **kwargs):
+        client = self.get_conn()
+
+        client.delete(hdfs, recursive=True)
+        model = {
+            '(intercept)': 48.,
+            'first_feature': 2.,
+            'second_feature': 12.,
+        }
+
+        with client.write(hdfs, encoding='utf-8') as writer:
+            for item in model.items():
+                writer.write(u'%s,%s\n' % item)
